---
title: "Customer_Experience_in_R"
author: "Reinp"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: 
    keep_md: yes
  word_document: default
---

# R Programming

## Set Chunk requirements

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
#echo=FALSE indicates that the code will not be shown in the final document 
#(though any results/output would still be displayed).
#include=FALSE to have the chunk evaluated, but neither the code nor its output displayed
# warning=FALSE and message=FALSE suppress any R warnings or messages from being included 
#in the final document
```

## Example 


```{R loading the data set}

# Importing the data.table
# ---
# 
library("data.table")
library(stats)
library(psych)
library(ggplot2)

# Reading our dataset
# ---
#fread accepts http and https URLs directly as well as operating system commands 
#such as sed and awk output.

hospitality_dt <- fread('http://bit.ly/HospitalityDataset')
View(hospitality_dt)
attach(hospitality_dt)

```


```{R r structure of data}
# What is the structure of the data?
# ---
# 
head(hospitality_dt)

```


```{r number of variables and observations}
# How many variables and observations are there?
# 
ncol(hospitality_dt)
nrow(hospitality_dt)


```

```{r learn more about the data set}
#learn more about the dataset
help(hospitality_dt)
??hospitality_dt
str(hospitality_dt)
class(hospitality_dt)
typeof(hospitality_dt) 
length(hospitality_dt)
names(hospitality_dt) #display variable names
#attributes(hospitality_dt) #names(hospitality_dt), class(hospitality_dt), row.names(hospitality_dt)

```


```{r}

summary(hospitality_dt)

#distribution of input variables
table(hospitality_dt$gender)
table(hospitality_dt$score)
table(hospitality_dt$branch)

```

```{R}
# What is the missing data?
# 
sum(is.na(hospitality_dt))

```


```{R}
# NB: Let's deal with "-" in our scores variable
# Assumption is that those customers did not fill in the survey
# 
hospitality_dt$score[hospitality_dt$score == "-"] <- NA

head(hospitality_dt)


```


```{R}
# Getting rid of missing data, check size and preview
# Size of original dataset was 296852
# 
hospitality_dt1 <- na.omit(hospitality_dt)
nrow(hospitality_dt1)
head(hospitality_dt1)

View(hospitality_dt1)
attach(hospitality_dt1)


str(hospitality_dt1)
class(hospitality_dt1)
typeof(hospitality_dt1) 
length(hospitality_dt1)
names(hospitality_dt1) #display variable names


summary(hospitality_dt1)

#distribution of input variables
table(hospitality_dt1$gender)
table(hospitality_dt1$score)
table(hospitality_dt1$amount)
table(hospitality_dt1$branch)


```


```{R}
# What is the overall proportion of repeat customers?
#duplicated() function uses logical values to determine duplicated values.  

#duplicated(hospitality_dt1$user_id)

sum(duplicated(hospitality_dt1$user_id))

dim(hospitality_dt1[duplicated(hospitality_dt1$user_id),])[1] #gives you number of duplicates

table(duplicated(hospitality_dt1$user_id))

mean(duplicated(hospitality_dt1$user_id))

sum(duplicated(hospitality_dt1$user_id)) / nrow(hospitality_dt1)


```


```{R}
# How many times do customers come back on average?


#unique() function uses numeric indicators to determine unique values.

library(plyr)

#unique(hospitality_dt1$user_id)

#count(unique(hospitality_dt1$user_id))

#table(unique(hospitality_dt1$user_id))

dim(hospitality_dt1[unique(hospitality_dt1$user_id),])[1] #gives you number of uniques




```


```{R}
# How many customers are repeat customers per branch?
#   
sum(duplicated(hospitality_dt1[,c('user_id','branch')]))


```


```{R}
# What is the NPS?
# 

# Importing our NPS library
# 
library(NPS)

# Converting score column to numeric
#
hospitality_dt1$score <- as.numeric(as.character(hospitality_dt1$score))

# Computing our NPS
nps(hospitality_dt1$score)


```


```{R}
# Here are the proportions of respondents giving each Likelihood to
# recommend response
#
prop.table(table(hospitality_dt1$score))


```


```{R histscores}
# Plotting a histrogram of the scores
# 

# Lets first import tidyverse
#
library(tidyverse)

hist(
  hospitality_dt1$score, breaks = -1:10,
  col = c(rep("red", 7), rep("yellow", 2), rep("green", 2))
)


```


```{R barplotscores}
# Here's a barplot. It's very similar, though for categorical responses
# it's often slightly easier to interpret
#
barplot(
 prop.table(table(hospitality_dt1$score)),
 col = c(rep("red", 7), rep("yellow", 2), rep("green", 2))
)


```


```{R ggplotnps}
# Is there a relationship between NPS segment and amount spent? 
#  
ggplot(hospitality_dt1, aes(x=score, y=amount)) + geom_point()



```

## Exercise


```{R uniqueidData}

#Build a data model with unique id only 


hospitality_dt1[!duplicated(hospitality_dt1$user_id),] #gives you unique rows



#Data with unique id only
hospitality_dt2u <- hospitality_dt1[!duplicated(hospitality_dt1$user_id),]
View(hospitality_dt2u)
attach(hospitality_dt2u)

nrow(hospitality_dt2u)

mean(hospitality_dt2u$amount)

# Converting score column to numeric
hospitality_dt2u$score <- as.numeric(as.character(hospitality_dt2u$score))

# Computing our NPS
nps(hospitality_dt2u$score)

# proportions of respondents giving each Likelihood to

prop.table(table(hospitality_dt2u$score))

#Histogram

hist(
  hospitality_dt2u$score, breaks = -1:10,
  col = c(rep("red", 7), rep("yellow", 2), rep("green", 2))
)

#Barplot

barplot(
 prop.table(table(hospitality_dt2u$score)),
 col = c(rep("red", 7), rep("yellow", 2), rep("green", 2))
)


ggplot(hospitality_dt2u, aes(x=score, y=amount)) + geom_point()
```

```{R}


#For the unique userID data: separate the genders, find the average amount spent, find average NPS
hospitality_dt2uF <- hospitality_dt2u[hospitality_dt2u$gender == "F"]
View(hospitality_dt2uF)
attach(hospitality_dt2uF)

head(hospitality_dt2uF)
nrow(hospitality_dt2uF)

mean(hospitality_dt2uF$amount)

# Converting score column to numeric
#
hospitality_dt2uF$score <- as.numeric(as.character(hospitality_dt2uF$score))

# Computing our NPS
nps(hospitality_dt2uF$score)

prop.table(table(hospitality_dt2uF$score))




hospitality_dt2uM <- hospitality_dt2u[hospitality_dt2u$gender == "M"]
View(hospitality_dt2uM)
attach(hospitality_dt2uM)

head(hospitality_dt2uM)
nrow(hospitality_dt2uM)

mean(hospitality_dt2uM$amount)

# Converting score column to numeric
#
hospitality_dt2uM$score <- as.numeric(as.character(hospitality_dt2uM$score))

# Computing our NPS
nps(hospitality_dt2uM$score)

prop.table(table(hospitality_dt2uM$score))

```


```{R}

#Add a column with the word 'repeat' for repeated user ID and 'non-repeat' for unique user ID

#Data with repeated id only

hospitality_dt1[duplicated(hospitality_dt1$user_id),] #gives you duplicate rows

hospitality_dt2r <- hospitality_dt1[duplicated(hospitality_dt1$user_id),]
View(hospitality_dt2r)
attach(hospitality_dt2r)

nrow(hospitality_dt2r)


#Whatever is on the left of the <- sign “gets” whatever is on the right

hospitality_dt2r$repeat_customer<-"repeat"
hospitality_dt2u$repeat_customer<-"non-repeat"


#To join two data frames (datasets) vertically
hospitality_dt1new <- rbind(hospitality_dt2r, hospitality_dt2u)
View(hospitality_dt1new)
attach(hospitality_dt1new)

head(hospitality_dt1new)
nrow(hospitality_dt1new)


str(hospitality_dt1new)
class(hospitality_dt1new)
typeof(hospitality_dt1new) 
length(hospitality_dt1new)
names(hospitality_dt1new) #display variable names


summary(hospitality_dt1new)

#distribution of input variables
table(hospitality_dt1new$gender)
table(hospitality_dt1new$score)
table(hospitality_dt1new$amount)
table(hospitality_dt1new$branch)
table(hospitality_dt1new$repeat_customer)


```


### Model (Gender-factor, score - numeric, amount - numeric)

```{R}
# Can we build a logistic regression model to predict 
# whether a customer will be a repeat customer or not?
# 

hospitality_dt1new$repeat_customer <- factor(hospitality_dt1new$repeat_customer, 
                                             levels = c("repeat","non-repeat"), 
                                      labels = c(0,1))



# Converting repeat_customer column to numeric


hospitality_dt1new$repeat_customer <- as.numeric(as.character(hospitality_dt1new$repeat_customer))

head(hospitality_dt1new)

hospnew.glm = glm(formula=repeat_customer ~ amount + score + gender , data = hospitality_dt1new,
                  family=binomial)
hospnew.glm

summary(hospnew.glm)


```

```{R}

#amount spent, score and male gender are significant.

#The logistic regression coefficients give the change in the log odds of the outcome for 
#a one unit increase in the predictor variable.

#For a one unit increase in amount spent, the change in log odds of being a repeat customer increases 
#by 0.0000707 holding all other variables constant.

#For every one unit change in score, the change in log odds of repeat (versus non-repeat) decreases 
#by -0.0443532 holding all other variables constant.

#The log odds of being a repeat male customer is 0.1300867 lower than the log odds of being a repeat 
#female customer holding all other variables constant.



#confidence intervals for the coefficient estimates
## CIs using profiled log-likelihood
confint(hospnew.glm)

## CIs using standard errors
confint.default(hospnew.glm)

```


```{R}

#exponentiate the coefficients and interpret them as odds-ratios

## odds ratios only
exp(coef(hospnew.glm))


#To put it all in one table, we use cbind to bind the coefficients and confidence intervals
#column-wise.

## odds ratios and 95% CI
exp(cbind(OR = coef(hospnew.glm), confint(hospnew.glm)))

```

```{R}

#For every one unit increase in amount spent, the odds of being a repeat customer 
#(versus non-repeat) increases by a factor of 1.0000707 holding all other variables constant.

#For every one unit change in score, the odds of being a repeat customer 
#(versus non-repeat) decreases by a factor of 0.9566160 holding all other variables constant.

#The odds of being a repeat male customer is 0.878 times the odds of being a repeat female customer
#holding all other variables constant. 

```

```{R}
#We may also wish to see measures of how well our model fits. This can be particularly 
#useful when comparing competing models.

#The output produced by summary(hospnew.glm) included indices of fit, 
#including the null and deviance residuals and the AIC. 

#One measure of model fit is the significance of the overall model. This test asks 
#whether the model with predictors fits significantly better than a model with just an 
#intercept (i.e., a null model). 

#The test statistic is the difference between the residual deviance for the model 
#with predictors and the null model. The test statistic is distributed chi-squared 
#with degrees of freedom equal to the differences in degrees of freedom between the 
#current and the null model (i.e., the number of predictor variables in the model).


#To find the difference in deviance for the two models (i.e., the test statistic), 
#we use the command:

with(hospnew.glm, null.deviance - deviance)


#The degrees of freedom for the difference between the two models is equal to the number of
#predictor variables in the model, and can be obtained using:

with(hospnew.glm, df.null - df.residual)

#the p-value can be obtained using:

with(hospnew.glm, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))


#To see the model’s log likelihood, we type:

logLik(hospnew.glm)

```

```{R}
#The chi-square of 113.5097 with 3 degrees of freedom and an associated p-value of less 
#than 0.001 (1.927131e-24) tells us that our model as a whole fits significantly better 
#than an empty model. This is sometimes called a likelihood ratio test.



#(the deviance residual is -2*log likelihood)
#-2*-17397.51=34795.02

#Predict the model

newdata = data.frame(amount=990 , score=10 , gender="M")

predict(hospnew.glm, newdata, type="response")

hospitality_dt1new$PredModel_Scorenumeric <-predict(hospnew.glm, hospitality_dt1new, type="response")

```


### Model (Gender-factor, score - factor, amount - numeric)

```{R}



#convert score to a factor to indicate that score should be treated as a categorical variable.

hospitality_dt1new$score <- factor(hospitality_dt1new$score)

hospnew1.glm = glm(formula=repeat_customer ~ amount + score + gender , data = hospitality_dt1new,
                  family=binomial)
hospnew1.glm

summary(hospnew1.glm)



#We can test for an overall effect of score using the wald.test function of the aod library. 

#The order in which the coefficients are given in the table of coefficients is the same 
#as the order of the terms in the model. 

#This is important because the wald.test function refers to the coefficients by their order 
#in the model. We use the wald.test function. b supplies the coefficients, while Sigma supplies 
#the variance covariance matrix of the error terms, finally Terms tells R which terms in the 
#model are to be tested, in this case, terms 3 to 12.

library(aod)

wald.test(b = coef(hospnew1.glm), Sigma = vcov(hospnew1.glm), Terms = 3:12)


#The chi-squared test statistic of 78.9, with 10 degrees of freedom is associated with 
#a p-value of 8.4e-13 indicating that the overall effect of score is statistically significant.


#We can also test additional hypotheses about the differences in the coefficients for the 
#different levels of score. 

#Below we test that the coefficient for score=4 is equal to the coefficient for score=5. 
#The first line of code below creates a vector l that defines the test we want to perform. 
#In this case, we want to test the difference (subtraction) of the terms for score=4 and 
#score=5 (i.e., the 6th and 7th terms in the model). To contrast these two terms, we multiply 
#one of them by 1, and the other by -1. The other terms in the model are not involved in the 
#test, so they are multiplied by 0. The second line of code below uses L=l to tell R that we 
#wish to base the test on the vector l (rather than using the Terms option as we did above).

l <- cbind(0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0)
wald.test(b = coef(hospnew1.glm), Sigma = vcov(hospnew1.glm), L = l)

#The chi-squared test statistic of 0.45 with 1 degree of freedom is associated with a p-value 
#of 0.5, indicating that the difference between the coefficient for score=4 and the coefficient 
#for score=5 is statistically not significant.


#Below we test that the coefficient for score=8 is equal to the coefficient for score=9.
m <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0)
wald.test(b = coef(hospnew1.glm), Sigma = vcov(hospnew1.glm), L = m)

#The chi-squared test statistic of 22.9 with 1 degree of freedom is associated with a p-value 
#of 1.7e-06, indicating that the difference between the coefficient for score=8 and the 
#coefficient for score=9 is statistically significant.


```



```{R}
#how well our model fits. This can be particularly 
#useful when comparing competing models.


#To find the difference in deviance for the two models (i.e., the test statistic), 
#we use the command:

with(hospnew1.glm, null.deviance - deviance)


#The degrees of freedom for the difference between the two models is equal to the number of
#predictor variables in the model, and can be obtained using:

with(hospnew1.glm, df.null - df.residual)

#the p-value can be obtained using:

with(hospnew1.glm, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))


#To see the model’s log likelihood, we type:

logLik(hospnew1.glm)


#The chi-square of 141.8541 with 12 degrees of freedom and an associated p-value of 
#less than 0.001 (2.52885e-24) tells us that our model as a whole fits significantly better 
#than an empty model. This is sometimes called a likelihood ratio test.



#(the deviance residual is -2*log likelihood)
#-2*-17383.34=34766.68

```

```{R}

#amount spent, Two terms of score (9 and 10) and male gender are significant.

#The logistic regression coefficients give the change in the log odds of the outcome for 
#a one unit increase in the predictor variable.

#For a one unit increase in amount spent, the log odds of being a repeat customer increases 
#by 0.00006983 holding all other variables constant.

#Ranking the Visit to Stony Hill coffee house with a score of 1 verses score of 0, changes 
#the log odds of being a repeat customer by -0.1601

#Visiting Stony Hill coffee house being male versus being female changes the log odds of 
#being a repeat customer by -0.1327.



#confidence intervals for the coefficient estimates
## CIs using profiled log-likelihood
confint(hospnew1.glm)

## CIs using standard errors
confint.default(hospnew1.glm)


#exponentiate the coefficients and interpret them as odds-ratios

## odds ratios only
exp(coef(hospnew1.glm))


#To put it all in one table, we use cbind to bind the coefficients and confidence intervals
#column-wise.

## odds ratios and 95% CI
exp(cbind(OR = coef(hospnew1.glm), confint(hospnew1.glm)))


#For every one unit increase in amount spent, the odds of being a repeat customer 
#(versus non-repeat) increases by a factor of 1.0000698


#Predict the model

# in data repeat (0)
newdata = data.frame(amount=990 , score="10" , gender="M")

predict(hospnew1.glm, newdata, type="response")

hospitality_dt1new$PredModel_Scorefactor <-predict(hospnew1.glm, hospitality_dt1new, type="response")

```






















